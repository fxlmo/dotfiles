file:///home/josh/Documents/formal-notes/year-3/IPCV/ipcv.tex {"mtime":1610717726751,"ctime":1610555896988,"size":29980,"etag":"36applsbi1063","orphaned":false}
% Preamble {{{
\documentclass[11pt,a4paper,titlepage,dvipsnames,cmyk]{scrartcl}
\usepackage[english]{babel}
\typearea{12}
% }}}

% Set indentation and line skip for paragraph {{{
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\usepackage[margin=2cm]{geometry}
\addtolength{\textheight}{-1in}
\setlength{\headsep}{.5in}
% }}}

\usepackage{hhline}
\usepackage[table]{xcolor}
\usepackage{mathtools}
\usepackage[T1]{fontenc}

% Headers setup {{{
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Image Processing and Computer Vision}
\rhead{Josh Felmeden}
\usepackage{hyperref}
% }}}

% Listings {{{
\usepackage[]{listings}
\lstset
{
    breaklines=true,
    tabsize=3,
    showstringspaces=false
}

\definecolor{lstgrey}{rgb}{0.05,0.05,0.05}
\usepackage{listings}
\makeatletter
\lstset{language=[Visual]Basic,
backgroundcolor=\color{lstgrey},
frame=single,
xleftmargin=0.7cm,
frame=tlbr, framesep=0.2cm, framerule=0pt,
basicstyle=\lst@ifdisplaystyle\color{white}\footnotesize\ttfamily\else\color{black}\footnotesize\ttfamily\fi,
captionpos=b,
tabsize=2,
keywordstyle=\color{Magenta}\bfseries,
identifierstyle=\color{Cyan},
stringstyle=\color{Yellow},
commentstyle=\color{Gray}\itshape
}
\makeatother
\renewcommand{\familydefault}{\sfdefault}
\newcommand{\specialcell}[2][c]{%
\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
% }}}

% Other packages {{{
\usepackage{graphicx}
\graphicspath{ {./pics/} }
\usepackage{needspace}
\usepackage{tcolorbox}
\usepackage{soul}
\usepackage{babel,dejavu,helvet}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{tcolorbox}
\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{enumitem}
\setlist{nolistsep}
% }}}

% tcolorbox {{{
\newtcolorbox{blue}[3][] {
colframe = #2!25,
colback = #2!10,
#1,
}

\newtcolorbox{titlebox}[3][] {
colframe = #2!25,
colback = #2!10,
coltitle = #2!20!black,
title = {#3},
fonttitle=\bfseries
#1,
}
% }}}

% Title {{{
\title{Image Processing and Computer Vision}
\author{Josh Felmeden}
% }}}

\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Image Acquisitions and Representation}
A human is able to register signals in a really small amount of time. When a human processes an image, it will be different to that of a computer. For example, a computer system might first try to do object detection. It would need some key information about objects so that it can categorise them. Once it is categorised, it can then do a higher level of processing ot work out the specific object it is, rather than an umbrella term.

The vision system could also do a rough 3D layout, with depth ordering. The system initially only gets a 2D flat image. If it can work out depth, then this may make it better.

\subsection{Challenges faced by a computer vision system}
\begin{itemize}
    \item View-point variation
    \begin{itemize}
        \item It has to be smart enough to recognise an object from any view point. For example, is a side view of a face enough?
    \end{itemize}
    \item Illumination
    \begin{itemize}
        \item The system needs to be able to recognise an object given any number of illuminations
    \end{itemize}
    \item Occlusion
    \begin{itemize}
        \item Sometimes an object may be slightly obscured, and not be fully visible. This is especially rough for image detection
    \end{itemize}
    \item Scale
    \begin{itemize}
        \item A close object will look larger than the same object from a distance
    \end{itemize}
    \item Deformation
    \begin{itemize}
        \item A non-rigid object can undergo shape deformation. The system needs to be able to cope with such changes
    \end{itemize}
    \item Background clutter
    \begin{itemize}
        \item There needs ot be some kind of foreground-background separation because the background might be quite noisy
    \end{itemize}
    \item Object intra-class variation
    \begin{itemize}
        \item Categories can be really vast and complex. This makes categorisation difficult for the system
    \end{itemize}
    \item Local ambiguity
    \begin{itemize}
        \item Blurring an image can be beneficial sometimes, but extracted information can be ambiguous because this makes it difficult to tell what object a certain pixel belongs to
    \end{itemize}
    \item The world behind the image
    \begin{itemize}
        \item Computer vision is now advanced enough to be able to take some 2D image and make a good guess at a pseudo 3D model.
    \end{itemize}
\end{itemize}

\subsection{Basics of Image Acquisition and Representation}
The pinhole camera is a classic example of this. The light passes through the pinhole and is represented as a projection. The pinhole needs to be an exact size. Too big, and the image is blurry. Too small, and too little light is let in.

If we take this example for computer vision, the image is projected into as \textbf{discrete sensor array}. This would be the sensor plane in the camera. Next, the image is sampled. The lower the resolution of the sample size, the lower the resolution of the images. Now, we have an approximation of the image from the real world.

\subsection{Modelling a Spatial BRightness Pulse}
The \textbf{Dirac Delta-Function} is really useful for this. It is utilised alongside the \textbf{sifting property}, and through these we are able to get a good image of the signal. This is useful for finding lines.

The function is defined as:
\begin{align*}
\int_{-\infty}^{\infty} \delta (t) dt = 1
\end{align*}

Intuitively:
\begin{align*}
\delta(t) = \lim_{\epsilon \rightarrow 0} [ y_\epsilon (t) ]
\end{align*}

The sifting property is:
\begin{align*}
\int_{-\infty}^{\infty} f(t) \delta(t) dt = f(0)
\end{align*}

Now, to extend this into two dimensions, we consider the 2D `image function' as a linear combination of 2D dirac pulses located at given points $a,b$ that cover the whole image plane.

Ideally, the optical system should be mapping point information from the scene to a point in the image. However, the world is not as ideal as we would like. Instead, we get a \textbf{point spread} which is the information about how the points are spread \textit{funnily enough}.

The \textbf{superposition principle} states that an image is the sum of the PSF of all its points. This means that an image is inherently a blurry version of the real world, no matter how sharp the image is to us. Even our eyes have this. Consider the fact that no matter how good an image is, it is still not the complete recreation of the real world thing.

\subsection{Modelling an image}
An image can be represented in \textbf{matrix form}. Each position in the matrix maps to a specific pixel in the image, and the value represents the colour (usually in RGB form, but in $f(x,y)$, an RGB cube). Other than the RGB cube, we have a double cone and this is called HSI representation:
\begin{itemize}
    \item \textbf{H} --- Hue
    \item \textbf{S} --- Saturation
    \item \textbf{I} --- Intensity
\end{itemize}

We will only be sticking with RGB for now, though.

\subsection{Quantization and image sampling}
Quantization represents a continuously varying single channel image function $f(x,y)$ with a discrete one using quantization levels.

The effect of very sparse sampling is often \textit{aliasing}. This is an effect that causes different signals to become indistinguishable (or aliases of on another) when sampled.

\textbf{Anti-aliasing} can be achieved by removing all spatial frequencies above a critical limit (called the Shannon-Nyquist limit).

\textbf{Shannon's sampling theorem} is `\textit{An analogue signal containing components up to some maximum frequency u may be completely reconstructed by regularly spread samples, provided the sampling rate is above 2u samples per second}'. TL;DR, the sampling must be performed above twice the highest spatial frequency of the signal for it to be lossless.

\section{Filtering Images}
\subsection{Image Transforms}
An \textbf{image transform} is a derivation of a \textit{new representation} of the input data space (such as Fourier). Image transforms are classic processing techniques used in filtering, compression, feature extraction and much more.

\subsection{Convolution}
Convolution is a mathematical operation on two functions ($f$ and $g$) that produces a third function ($h=f*g$), representing how the shape of one is modified by the other.

\begin{align*}
f*g = \int_{-\infty}^{\infty} f(x-t) g(x) \delta t
\end{align*}

\begin{itemize}
    \item Used for filtering images in the spatial domain
    \item Application in other parameter spaces
    \item Fundamental to Convolutional Neural Nets for deep learning
    \item And so on
\end{itemize}

The discrete version of 2D convolution is defined as
\begin{align*}
g(x,y) = \sum_{m} \sum_{n} f(x-m,y-n) h(m,n)
\end{align*}

\textbf{NOTE!} Correlation is the same as convolution when the kernel is \textit{symmetric} under $180^o$ rotation.

\subsubsection{Normalising the Convolution Result}
The weights will affect pixel values and the result could then fall outside of hte traditional 0-255 range. Therefore, we need to \textit{normalise} the results:
\begin{itemize}
    \item If all weights are positive, then normalise them such that they \textbf{sum to 1}.
    \item If they are both positive and negative weights, they should \textbf{sum to 0} \textit{but not always}.
\end{itemize}

\subsubsection{Gaussian filter kernel}
The \textit{Gaussian} filter is very commonly used in image processing. The parameter $\sigma$ determines the width of the filter and hence the amount of smoothing. It is defined as follows:
\begin{align*}
g(x) = \frac{1}{2 \pi \sigma}e^{-\frac{x^2}{2\sigma^2}}
\end{align*}

Applying this filter blurs the image. Filtering with the 2D gaussian can actually be achieved faster using two 1D gaussian filters. This is because the linear Gaussian kernel is \textit{separable}. This means that a 2D convolution can be reduced to the product of two 1D convolutions (one on rows and one on columns). This is a significant reduction in computations.

\subsubsection{Noise removal}
To remove noise, we will be looking at the \textbf{median filter}. It returns the median value of the pixels in a neighbourhood and is non-linear. It is also similar to a uniform blurring filter, which returns the mean value of the pixels in a neighbourhood of a pixel. The algorithm is as follows

\begin{enumerate}
\item Let $I$ be a monochrome image
\item Let $Z$ define a neighbourhood of arbitrary shape
\item At each pixel location $p=(x,y)$ in $I$....
\item ... select the $n$ pixels in the ma   Z-neighbourhood of $p$
\item ... sort the $n$ pixels in the neighbourhood of $p$ by value into a list  $L(j)$ for $j = 1, \dots, n$
\item The output value at $p$ is $L(m)$ where $m = \lfloor n/2 \rfloor +1$
\end{enumerate}

\subsection{Sharpening}
Blurring takes away some of the sharpness of the image, so we acan add back the details to make the image sharper.

To recap, images are often corrupted by random variations in intensity, illumination, or have poor contrast and cannot be used directly. Filtering allows us to achieve:
\begin{itemize}
    \item \textit{Enhancement} to improve contrast
    \item \textit{Smoothing} to remove noise
    \item \textit{Template matching} to detect known patterns
    \item \textit{Feature extraction} to provide clues about objects for further analysis.
\end{itemize}

\section{Frequency Domain and Transforms}
Frequency allows us to characterise signals. They repeat over regular intervals with frequency $u = \frac{1}{T}$ cycles per second (Hz), amplitude ($a$, peak value) and the phase, $\Theta$ (shift in degrees).

\subsection{Fourier's Theorem}
Fourier's equation is as follows:
\begin{align*}
f(x) = \int a_n \cos(nx) + b_n \sin(nx) \delta n
\end{align*}

The sines and cosines are the \textbf{basic functions} of this representation. $a_n$ and $b_n$ are the \textbf{Fourier coefficients}. The sinusoids are harmonically related: each one's frequency is an integer multiple of the fundamental frequency of the input signal. 

Pixels that change slowly have a low frequency. Rapidly changing pixels have a high frequency.

The Fourier transform of a continuous function of two variables $f(x,y)$ is:
\begin{align*}
F(u,v) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x,y) e^{-i2 \pi (2ux + vy)} dxdy
\end{align*}

Conversely, given $F(u,v)$ we can obtain $f(x,y)$ by means of the inverse Fourier Transform:
\begin{align*}
f(x,y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} F(u,v) e ^{i 2\pi (ux + vy)} dudv
\end{align*}

\begin{tcolorbox}
These two equations are also known as the \textbf{Fourier Transform Pair.} They constitute a lossless representation of data.
\end{tcolorbox}

In the first equation, $f(x,y)$ is the image, and the $e$ function is the kernel (probing function).

\textbf{Euler's formula} is as follows:
\begin{align*}
e^{i \theta} \cos \theta + i \sin \theta
\end{align*}

Therefore, as each term of the Fourier Transform is composed of the sum of all values of the image function multiplied by a particular kernel at a particular frequenct and orientation specified by $(u,v)$:
\begin{align*}
F(u,v) = \frac{1}{N^2} \sum_{x=0}^{N-1}\sum_{y=0}^{N-1} f(x,y) \bigg [ \cos \bigg ( \frac{2 \pi(ux + vy)}{N}\bigg) - i \sin \bigg( \frac{2\pi (ux+vy)}{N} \bigg ) \bigg ]
\end{align*}

All kernels together form a new orthogonal basis for our image. 

\subsubsection{The Frequency Domain}
\begin{itemize}
    \item $F(u,v)$ is a complex number and has both real and imaginary parts: $F(u,v) = R(u,b) + iI(u,v)$
    \item Magnitudes (forming the power spectrum): $|F(u,v)| = \sqrt(R^2(u,v) + I^2(u,v))$
    \item Phase angles (forming the phase spectrum): $\theta (u,v) = \tan^{-1} [I(u,v)/R(u,v)]$
    \item Expressing $F(u,v)$ in polar coordinates $(r,\theta)$: $F(u,v) = |F(u,v)|e^{i \theta(u,v)} = re^{i\theta}$
\end{itemize}

An important property of the Fourier Transform is that the Fourier Transform of a real function $f(x,y)$ gives us:
\begin{align*}
F(u,v) = F^* (-u,-v) \Rightarrow |F(u,v)| = |F^*(-u,-v)|
\end{align*}

We can relate frequencies to images too. If we look at the fourier space, we can remove some of the frequencies to blur the images. Additionally, the low frequencies represent the contrast while high frequencies represent details.

\textbf{Phase} is important since the loss of \textit{magnitude} is teh loss of contrast, while even if we lose phase, we still retain positional arguments and edges.

\textbf{Convolution} in the spatial domain is equal to multiplication in the frequency domain and vice versa. This is useful because multiplication can be easier and faster than convolution. 

Elements that are scaled up have their frequency decreased. There is a \textbf{inverse scaling relationship}.

\newpage
\section{Edge Detection}
Images can be stored as matrices of numbers. Shapes are much more informative than the individual pixel values because we, as humans, don't look at individual pictures, rather the shapes themselves. We are also able to draw objects with just the edges and be able to recognise the object. This is through \textit{edge detection}. 

\begin{itemize}
    \item Edges highlight the \textbf{contour} of shapes
    \item They can be used to identify objects, but are not always easy to see.
    \item Thankfully, we have a variety of different techniques that are useful in different situations.
    \item Some of the more complicated solutions are the edges that can be produced via \textbf{optical illusions}.
\end{itemize}

Detecting edges is really useful for a number of reason:
\begin{itemize}
    \item Segmentation
    \begin{itemize}
        \item Assigning each pixel to a specific category
        \item Useful in self driving cars
    \end{itemize}
    \item Recognition
    \begin{itemize}
        \item Associate a specific level to an object.
        \item This allows computers to guess at what an image could contain.
        \item The image with the strongest connection is the estimation.
    \end{itemize}
    \item Motion analysis
    \begin{itemize}
        \item `Pinning' a sticker on snapchat or Instagram is an example of this
        \item It works really well on objects with sharp edges, but one with little contrast makes this really hard
    \end{itemize}
\end{itemize}

There are two main types of edge, \textbf{Meaningful} edges, and \textbf{nuisance} edges. The latter are edges that do not add to the \textit{understanding} of the image.

\subsection{Edge Detection Strategies}
We can take the \textbf{derivative} of an image to measure the change of the pixels in the image. This is what's known as the \textbf{change vector}. The gradient points towards the direction of change, which is perpendicular to the edge itself.

To extract the gradient, we apply \textit{filtering}.

With a noisy image, simply measuring the derivative would actually \textit{increase} the noise, because this magnifies the changes between the individual pixels, so instead we use a \textbf{kernel}.

\subsubsection{Image Gradients}
Changes in the neighbourhood can be visualised using derivations in 2D space. A vector variable consists of:
\begin{itemize}
    \item Direction $\phi$ of the maximum growth of the function
    \item Magnitude $|\nabla f(x,y)$ of the growth
    \item Perpendicular to the edge direction
\end{itemize}

The derivatives are the components of the gradient vector. Edge vectors are in the \textit{perpendicular} direction to the gradient vector (as it states the direction of the edge). Gradient extraction can occur via filtering. We consider the nearby pixels using these matrices (for horizontal and vertical respectively).

\begin{align*}
\frac{\delta f}{\delta x} \simeq \begin{bmatrix}
    -1 & 0 & 1 \\
    -1 & 0 & 1 \\
    -1 & 0 & 1
\end{bmatrix}
\quad
\frac{\delta f}{\delta y} \simeq
\begin{bmatrix}
    -1 & -1 & -1 \\
    0 & 0 & 0 \\
    1 & 1 & 1
\end{bmatrix}
\end{align*}

Sequential derivatives do different things with noise, which is typically suppressed.

\subsubsection{Prewitt Operator/Sobel Operator}
To known what the best kernel to use is, we use the Sobel Operator. It gives stronger weight to the central pixels, since the closer they are to the centre, the more impact they should have on the derivative. It can be approximated as the derivative of the Gaussian:

\begin{align*}
    \frac{\delta}{\delta x} \simeq
\begin{bmatrix}
    -1 & 0 & 1 \\
    -2 & 0 & 2 \\
    -1 & 0 & 1
\end{bmatrix}
\quad
\frac{\delta}{\delta y} \simeq
\begin{bmatrix}
    -1 & -2 & -1 \\
    0 & 0 & 0 \\
    1 & 2 & 1
\end{bmatrix}
\end{align*}

As seen here, these kernels give stronger bias to the central elements. It is approximately a gaussian smoothing and derivation. This kernel is especially good at suppressing noise.

\subsection{Shape detection}
\subsubsection{Line Representation}
A straight line in 2D space is described by the equation:
\begin{align*}
f(x,y,\rho_0, \theta_0) = x \cos \theta_0 + y\sin \theta_0 - \rho_0 = 0
\end{align*}

Here, $\rho_0$ is the distance between the straight line and the origin, and $\theta_0$ is the angle between the distance vector and the positive $x$ direction.

\subsubsection{Hough Space}
A point $(x_0, y_0)$ in the image space is transformed into a sinusoidal curve in the parameter space. A point $(\theta, \rho)$ on this sinusoidal curve represents a straight line passing through the point $(x_0, y_0)$ in the image space.

We take a single point in the image and measure the pixel intensities on each line from the pixel. We sum all the pixels on each line and get a transformation to put into the Hough space. The result of this action is a new image. Where the pixels line up, the intensity will be brighter, and this is reflected in the Hough space. Now, we look for the peaks in the Hough space to identify the lines (since \textbf{peaks} in the Hough space correlate to lines).

The line detection algorithm is as follows:
\begin{enumerate}
\item Make an $n=2$ array $H(\rho, \theta)$ for the parameter space
\item Find the gradient image $G(x,y) = |G (x,y)| \angle G(x,y)$
\item For any pixel satisfying $|G(x,y)| > T_s$, increment all elements on the curve $\rho = x\cos\theta + y\sin \theta$ in the parameter space represented by the $H$ array:
\begin{gather*}
\forall \theta \quad | \quad \rho = x \cos \theta + y \sin \theta \\
H(\rho, \theta) = H (\rho, \theta + 1)
\end{gather*}
\item In the parameter space, any element $H(\rho, \theta > T_h)$ represents a straight line detected in the image.
\end{enumerate}

This algorithm can then be improved by making use of the gradient direction $\angle G$ which, in this particular ase is the same as the angle $\theta$. Now, for any point $|G(x,y)| > T_s$ we only need to increment the elements on a small segment of the sinusoidal curve. The third step in the algorithm can be modified as:
\begin{enumerate}
\item Make $n = 2$ dimensional array $H(\rho, \theta)$
\item Find the gradient image $G(x,y) = |G (x,y)| \angle G(x,y)$
\item For any pixel satisfying $|G(x,y)| > T_s$, increment all elements on the curve $\rho = x\cos\theta + y\sin \theta$ in the parameter space represented by the $H$ array:
\begin{gather*}
\forall \theta \quad | \quad \angle G(x,y) - \triangle \theta \le \theta \le \angle(x,y) + \triangle \theta \\
\rho = x \cos \theta + y \sin \theta \\
H(\rho, \theta) = H (\rho, \theta + 1)
\end{gather*}
\item In the parameter space, any element $H(\rho, \theta > T_h)$ represents a straight line.
\end{enumerate}

Now that we have an algorithm to detect lines, we can use a similar idea to look at circles. However, with circles, there are \textbf{three} variables to consider: their x and y coordinates and their \textbf{radii}.

\newpage
The algorithm would be something like:
\begin{enumerate}
\item For any pixel satisfying $|G(x,y)| > T_s$, increment all elements satisfying the two simultaneous equations:
\begin{gather*}
\forall r, \quad \bigg \{ \begin{matrix}
    x_0 = x \pm r \cos \angle G \\
    y_0 = y \pm r \sin \angle G
\end{matrix} \\
H (x_0,y_0,r) = H(x_0,y_0,r) + 1
\end{gather*}
\item In the parameter space, any element $H(x_0,y_0,r) > T_h$ represents a circle with radius $r$ located at $(x_0,y_0)$ in the image.
\end{enumerate}

So, for every circle we find, we sum up all of the intensities for the circles and the peaks represent the most likely place that a circle would be. the space for circles now becomes 3D, which needs to be explored for every individual pixel, which is a bit much. Therefore, we can encode shapes generally.

\subsubsection{Generalised Hough Transform}
If we don't have an analytical form of the targeted shape, we need an approximation by calculating $\theta, \phi$ in $k$ points:
\begin{enumerate}
\item Prepare a table with $k$ entries each indexed by an angle $\phi_i, (i - 1, \dots, k), \triangle \phi = 180/k$
\item Define a reference point $(x_c, y_c)$, (the centre of mass) $\forall P(x,y)$ on the boundary of the shape, find:
\begin{gather*}
\bigg \{ \begin{matrix}
    r = \sqrt{(x-x_c)^2 + (y-y_c)^2} \\
    \beta = \tan^{-1} (y-y_c)/(x-x_c)
\end{matrix}
\end{gather*}
and the gradient direction $\angle G$. Add the pair $(r, \beta)$ to the table entry with its $\phi$ closest to $\angle G$
\item Prepare a 2D Hough array $H(x_c,y_c)$ initialised to 0
\end{enumerate}

Now, we can look at the generalised Hough transform:
\begin{enumerate}
\item For each image point $(x,y)$ with $|G(x,y)| > T_s$ find the table entry with its corresponding angle $\phi_j$ closest to $\angle G(x,y)$
\item For each of the $n_j$ pairs $(r,\beta)_i(i=1, \dots, n_j)$ in this table entry, find:
\begin{gather*}
\bigg \{ \begin{matrix}
    x_c = x + r \cos \beta \\
    y_c = y + r \sin \beta
\end{matrix}
\end{gather*}
\item Increment the corresponding element in the $H$ array by 1:
\begin{gather*}
H(x_c,y_c) = H(x_c, y_c) + 1
\end{gather*}
\end{enumerate}
All element in the $H$ table satisfying $H(x_c,y_c) > T_h$ represents the location of shapes in the image.

\newpage
\subsubsection{Invariant Generalise Hough Transform}
The algorithms we have discussed thus far do not take into account orientation and scale, which is desirable. Therefore, we can add two additional parameters: a scaling factor $S$ and a rotational angle $\theta$. So, to finalise, $\forall P(x,y)$ with $|G(x,y)| > T$ find the table entry with its corresponding angle $\phi_j$ closest to $\angle G(x,y)$

Then, for each of the $n_j$ pairs $(r,\beta)_i(i=1, \dots, n_j)$ in this table entry, find:
\begin{gather*}
\bigg \{ \begin{matrix}
    x_c = x + r S \cos (\beta + \theta) \\
    y_c = y + r S \sin (\beta + \theta)
\end{matrix}
\end{gather*}

And finally, increment the corresponding element in the 4D $H$ array by 1:
\begin{gather*}
H(x_c,y_c, S, \theta) += 1
\end{gather*}

\section{Image Segmentation}
This is the process of spatial subsectioning of a digital image into \textit{multiple partitions of pixels} according to given criteria.

We might want to segment images for a number of regions:
\begin{itemize}
    \item \textbf{Simplify} an image
    \item \textbf{Higher-level object description}
    \item \textbf{Input} for content classifiers.
\end{itemize}

Perfect segmentation is really hard to fully achieve. A pixel might, for example, straddle the \textit{real} boundary of objects so it partially belongs to two or more objects. Noise, non-uniform illumination also makes the task more difficult.

\textbf{Over segmentation} means that the pixels belonging to the same region are classified as \textit{different objects}.

\textbf{Under segmentation} means that the pixels belonging to different regions are classified under the \textit{same region}.

\subsection{Concepts of segmentation}
\begin{itemize}
    \item \textbf{Thresholding methods} are when pixels are categorised based on intensity. This is only useful when sufficient contrast exists.
    \item \textbf{Edge based methods} are when region boundaries are
    constructed from edge maps.
    \item \textbf{Region based methods} are when regions grow from seed
    pixels. Region splitting and merging occurs for efficient spatial
    encoding.
    \item \textbf{Clustering and statistical methods} are global, often histogram based image partitioning, such as \textit{K-means}.
\end{itemize}

\subsubsection{Thresholding example}
If the image contains a dark object on a light background then:
\begin{itemize}
    \item Choose a threshold value $T$
    \item For each pixel
    \begin{itemize}
        \item If the brightness at that pixel is less than $T$, it is a pixel of interest
        \item Otherwise it is part of the background
    \end{itemize}
\end{itemize}

The value of the threshold is really important
\begin{itemize}
    \item If it is too high, then background pixels may be classified as foreground
    \item If it is too low, foreground pixels could be classified as background
\end{itemize}

To find a good threshold we can use an \textbf{image histogram}. This consists of:
\begin{itemize}
    \item Counting how many pixels in the image have each value
    \item For simple images, it shows peaks and valleys around regions of the image
\end{itemize}

\subsubsection{Thresholding selection algorithms}
\begin{enumerate}
\item Select an initial estimate for the threshold $T$.
\item Segment the image using $T$. This will produce two groups of pixels: $G_1$, consisting of all the pixels with grey levels that are greater than $T$, and $G_2$, consisting of the others.
\item Now, compute the average grey level values $m_1, m_2$ for the pixels in regions $G_1, G_2$.
\item Compute the new threshold value: $T=(m_1 + m_2)/2$
\item Repeat steps 2 through 4 until convergence.
\end{enumerate}

We can also make use of a divide and conquer approach to this. At the heart of this idea is the use of a \textbf{homogeneity function} $H$. If there is little variation, it returns 1. If there is a lot, it returns 0. Using this, we can instead merge all subregions that satisfy that they are homogeneous. We split each image in half until no more splitting can be done. This tends to result with the outcome becoming quite blocky, but makes it quite simple.

\begin{enumerate}
\item Start with $R_0$ that represents the entire image
\item If $H(R_i) = 0$ (it is not homogeneous), then split the area into 4 blocks and process each area again.
\item Merge all subregions that satisfy $H(R_i \cup R_j) = 1$ (are homogeneous).
\end{enumerate}

Conceptually, we simply decompose an image into regions of a selected shape that do not satisfy a homogeneity condition (\textit{split step}). Then, merge the regions that satisfy the homogeneity condition (\textit{merge step}).

\textbf{Clustering} is another method we can use. The crux of this method is to minimise this function:
\begin{gather*}
\Theta(clusters, data) = \sum_{j \in clusters} \Bigg [ \sum_{i \in j^{th} cluster} \|\mathbf{x}_i - \mathbf{\mu}\|^2 \Bigg]
\end{gather*}

The \textit{K-Means} algorithm goes something like:
\begin{enumerate}
\item Randomly initialise $K$ vectors $\mu_1 \dots \mu_K$
\item Assign each $x \in Features$ to the nearest $\mu_k$
\item Recompute each $\mu_j$ as the mean of the features assigned to it
\item Repeat steps 2 and 3 until there is no change in $\mu_1, \dots, \mu_K$
\end{enumerate}

This method attempts to find a configuration that minimises \textbf{within-cluster scatter}. This is teh total squared distance between point $x_i$ and centroid $\mu_j$ in the $j^th$ cluster. It's equivalent to maximising the between-cluster scatter. However, it is important to note that it \textit{may not} find a \textbf{global optimum}.

\section{Object detection (Viola and Jones' real-time method)}
Object detection aims to bridge the gap between
\begin{itemize}
    \item Given pixel values
    \item Meaningful objects
\end{itemize}

Image regions need to be found and assigned semantic labels from a space of object classes. 

In the real world, we face problems such as:
\begin{itemize}
    \item High \textbf{intra-class}, low \textbf{inter-class} variance.
    \begin{itemize}
        \item \textit{High intra-class} variance means that there's a great difference within a class (dog breeds for example)
        \item \textit{Low inter-class} variance means that there's a small difference between classes (such as chihuahua and rat).
    \end{itemize}
    \item Again, change of illumination, scale, pose, and occlusion make the task even more difficult.
\end{itemize}

\subsection{Sliding Window Detectors}
The basic idea of this is:
\begin{itemize}
    \item Image is tested for object presence window-by-window
    \item The window is `slid' and `scaled' throughout the image
    \item Each resultant window is judged wrt an object model, giving a response indicating object presence or absence.
\end{itemize}

\subsubsection{Haar-like features as weak classifiers}
Instead of using a single feature, we can use Haar-like features as different
classifiers. This is a simple rectangle and measures the sum of pixels of
areas inside them. The values produced indicate certain characteristics of a
particular area of the image. Each feature type can indicate the existence of
the characteristics in the image, such as edges or changes in texture. For
example, a 2-rectangle feature can indicate where the border lies between a
dark region and a light region

\end{document}